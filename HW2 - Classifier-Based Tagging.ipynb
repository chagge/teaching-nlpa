{
 "metadata": {
  "name": "HW2 - Classifier-Based Tagging"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import tagutils; reload(tagutils)\n",
      "from tagutils import *\n",
      "from IPython.core.display import HTML\n",
      "from nltk.corpus import brown\n",
      "import random as pyrand\n",
      "from tagutils import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Evaluation Framework"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sents = list(brown.tagged_sents())\n",
      "n = len(sents)\n",
      "test = sorted(list(set(range(0,n,10))))\n",
      "training = sorted(list(set(range(n))-set(test)))\n",
      "training_set = [sents[i] for i in training]\n",
      "test_set = [sents[i] for i in test]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(training_set)\n",
      "print len(test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "51606\n",
        "5734\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t0 = nltk.DefaultTagger('NN')\n",
      "t1 = nltk.UnigramTagger(training_set, backoff=t0)\n",
      "t2 = nltk.BigramTagger(training_set, backoff=t1)\n",
      "t2.evaluate(test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.9236947791164659"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classifier-Based Tagging"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.tag.api\n",
      "# help(nltk.tag.api.TaggerI)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implement a new tagger based on classifiers.\n",
      "\n",
      "When applying a classifier, you need to transform the input into a feature vector.\n",
      "In this case, we are trying to predict $P(t_n| < \\hbox{input words} > )$.  How do we do this?\n",
      "\n",
      "For a simple unigram tagger, we are estimating $P(t_n | w_n)$.\n",
      "If $w_n \\in V = \\\\{1,...,N\\\\}$, where $V$ is a vocabulary of size $N$ representing each word\n",
      "as an integer, then the input feature vector might be a binary vector $\\vec{x} = (x_1 ... x_N)$ where\n",
      "\n",
      "$$ x_i = \\delta_{i,w_n} $$\n",
      "\n",
      "For a simple bigram tagger, we are estimating something like $P(t_n | w_n t_{n-1})$, which\n",
      "we could similarly represent as a concatenation of two large binary input vectors.\n",
      "\n",
      "However, such a brute force approach may not work very well because we have a very high\n",
      "dimensional input vector and classifiers often need a lot of training data.\n",
      "We are free to preprocess the data in any form we like in order to get better feature\n",
      "vectors. \n",
      "\n",
      "Here are some ideas:\n",
      "\n",
      "- use the posterior probabilities for tags returned by a unigram and bigram tagger as feature vectors\n",
      "- use possible grammatical categories and semantic categories from Wordnet as feature vectors\n",
      "- use simple features like capitalization, word length, and position in sentence\n",
      "- provide information about word frequency in input\n",
      "- \"hash\" the large range of possible words $V$ down to a much smaller vocabulary\n",
      "- same as before, but do the hasing somewhat more intelligently: leave all the stop words alone, but has down the content words\n",
      "- do the \"hashing\" in some way that's informed by Wordnet\n",
      "\n",
      "Note that in order to be able to tag using the algorithms we have described, you can use tags assigned to previous words, but you cannot use tags assigned to subsequent words.\n",
      "\n",
      "Try to beat the bigram-with-backoff tagger above.\n",
      "\n",
      "Two classifiers to try are logistic regression and decision tree classifiers.\n",
      "You can use implementations from the `sklearn` package."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}