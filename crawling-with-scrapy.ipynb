{
 "metadata": {
  "name": "crawling-with-scrapy"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scrapy.spider import BaseSpider"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MySpider(BaseSpider):\n",
      "    name = \"uni-kl.de\"\n",
      "    allowed_domains = [\"uni-kl.de\"]\n",
      "    start_urls = [\n",
      "        \"http://www.uni-kl.de/\",\n",
      "    ]\n",
      "\n",
      "    def parse(self, response):\n",
      "        filename = response.url.split(\"/\")[-2]\n",
      "        open(filename, 'wb').write(response.body)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# see: http://stackoverflow.com/questions/7993680/running-scrapy-tasks-in-python\n",
      "\n",
      "from scrapy import project, signals\n",
      "from scrapy.conf import settings\n",
      "from scrapy.crawler import CrawlerProcess\n",
      "from scrapy.xlib.pydispatch import dispatcher\n",
      "from multiprocessing.queues import Queue\n",
      "from multiprocessing import Process\n",
      "\n",
      "class CrawlerWorker(Process):\n",
      "    def __init__(self, spider, results):\n",
      "        Process.__init__(self)\n",
      "        self.results = results\n",
      "\n",
      "        self.crawler = CrawlerProcess(settings)\n",
      "        if not hasattr(project, 'crawler'):\n",
      "            self.crawler.install()\n",
      "        self.crawler.configure()\n",
      "\n",
      "        self.items = []\n",
      "        self.spider = spider\n",
      "        dispatcher.connect(self._item_passed, signals.item_passed)\n",
      "\n",
      "    def _item_passed(self, item):\n",
      "        self.items.append(item)\n",
      "\n",
      "    def run(self):\n",
      "        self.crawler.crawl(self.spider)\n",
      "        self.crawler.start()\n",
      "        self.crawler.stop()\n",
      "        self.results.put(self.items)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The part below can be called as often as you want\n",
      "results = Queue()\n",
      "crawler = CrawlerWorker(MySpider(), results)\n",
      "crawler.start()\n",
      "for item in results.get():\n",
      "    print item"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}