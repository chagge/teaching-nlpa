{
 "metadata": {
  "name": "nltk-ngram-taggers"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import tagutils; reload(tagutils)\n",
      "from tagutils import *\n",
      "from IPython.core.display import HTML\n",
      "from nltk.corpus import brown\n",
      "import random as pyrand\n",
      "from tagutils import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Training and Test Sentences"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In previous worksheets, we looked at applying taggers to the analysis of texts.\n",
      "\n",
      "We already noted that taggers are often trained.\n",
      "\n",
      "Let's look in more detail at how taggers work.\n",
      "\n",
      "We start off by defining a `training_set` and a `test_set`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sents = list(brown.tagged_sents())\n",
      "pyrand.shuffle(sents)\n",
      "training_set = sents[:-1000]\n",
      "test_set = sents[-1000:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML(mstags(test_set[:5]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "`` <font color='#00c080''>So</font> <font color='#8080ff' size=+1>you're</font> <font color='red' size=+2>looking</font> <font color='#c08000' size=+2>for</font> <font size=-2>a</font> <font color='blue' size=+1>woman</font> who <font color='red' size=+2>married</font> <font size=-2>a</font> <font color='blue' size=+1>man</font> who <font color='red'>might</font> <font color='red'>have</font> <font color='red' size=+2>lived</font> <font color='#00c080''>here</font> <font size=-2>a</font> <font color='blue' size=+1>year</font> <font color='#00c080''>ago</font> <font color='purple' size=+1>and</font> <font color='red'>might</font> <font color='red'>have</font> been <font color='red' size=+2>poisoned</font> .<p />\n",
        "Five <font color='blue' size=+1>days</font> <font color='#00c080''>later</font> , <font color='#c08000' size=+2>on</font> <font color='red' size=+2>receiving</font> <font color='#8080ff' size=+1>it</font> , <font color='blue' size=+1>Meredith</font> <font color='red' size=+2>sat</font> <font color='red' size=+2>drumming</font> <font color='#8080ff' size=+1>his</font> <font color='blue' size=+1>dactyls</font> <font color='#c08000' size=+2>on</font> <font color='#8080ff' size=+1>his</font> <font color='red' size=+2>writing</font> <font color='blue' size=+1>table</font> .<p />\n",
        "The <font color='green'>Medical</font> <font color='blue' size=+1>Museum</font><p />\n",
        "<font size=-2>The</font> <font color='blue' size=+1>combination</font> <font color='red' size=+2>proved</font> quite <font color='green'>irresistible</font> last <font color='blue' size=+1>night</font> .<p />\n",
        "<font color='blue' size=+1>Bullets</font> <font color='red' size=+2>began</font> to <font color='red' size=+2>snap</font> <font color='#c08000' size=+2>past</font> <font color='#8080ff' size=+1>him</font> ."
       ],
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "<IPython.core.display.HTML at 0x38a5dd0>"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Unigram Tagging"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a variety of tools for keeping statistics.\n",
      "\n",
      "`FreqDist` counts the number of occurrences of its arguments and returns them\n",
      "in order of decreasing frequency."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd = nltk.FreqDist([w[0] for s in training_set for w in s])\n",
      "fd.items()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "[('the', 61543),\n",
        " (',', 57205),\n",
        " ('.', 48490),\n",
        " ('of', 35404),\n",
        " ('and', 27399),\n",
        " ('to', 25268),\n",
        " ('a', 21449),\n",
        " ('in', 19152),\n",
        " ('that', 10051),\n",
        " ('is', 9826)]"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`ConditionalFreqDist` computes statistics over pairs `(A,B)` and can be used to\n",
      "estimate $P(B|A)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that `cfd[word]` is essentially a posterior probability distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd = nltk.ConditionalFreqDist([w for s in training_set for w in s])\n",
      "cfd[\"frequent\"].items()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[('JJ', 32), ('VB', 2)]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a unigram model, we compute \n",
      "\n",
      "$$\\hbox{tag}(\\hbox{word}) = \\arg\\max P(\\hbox{tag}|\\hbox{word})$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likely_tags = dict((w, cfd[w].max()) for w in fd.keys())\n",
      "likely_tags.items()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[('Ranavan', 'NP-TL'),\n",
        " ('fawn', 'NN'),\n",
        " ('gai', 'FW-JJ'),\n",
        " ('mid-week', 'NN'),\n",
        " ('1,800', 'CD'),\n",
        " ('deferment', 'NN'),\n",
        " ('Debts', 'NNS-TL'),\n",
        " ('Poetry', 'NN'),\n",
        " ('woods', 'NNS'),\n",
        " ('clotted', 'VBN')]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now construct a simple tagger. This tagger just assigns the most\n",
      "frequent tag from the training set to each word."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseline_tagger = nltk.UnigramTagger(model=likely_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now evaluate performance on the test set; the error rate of\n",
      "this \"baseline tagger\" is about 10.3%."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseline_tagger.evaluate(test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "0.8969654199011997"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that performance on the training set is, as usual, better than on the test set.\n",
      "We've already seen this phenomenon in other pattern recognition systems."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseline_tagger.evaluate(training_set[:1000])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "0.9264958945867545"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "n-Gram Taggers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unigram taggers can also be trained and evaluated directly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unigram_tagger = nltk.UnigramTagger(training_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unigram_tagger.evaluate(test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "0.8969654199011997"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A generalization of unigram taggers are n-gram taggers.\n",
      "\n",
      "A bigram tagger computes tags as:\n",
      "\n",
      "$$\\hbox{T}(\\hbox{W}) = \\arg\\max_W P(T|W,W_\\hbox{prev})$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_tagger = nltk.BigramTagger(training_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_tagger.evaluate(test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "0.32364149611856036"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_tagger.evaluate(training_set[:1000])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "0.8145926545061213"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trigram_tagger = nltk.TrigramTagger(training_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trigram_tagger.evaluate(test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "0.16287932251235004"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trigram_tagger.evaluate(training_set[:1000])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "0.7679826933477556"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that for bigram and trigram taggers, performance in the test set is\n",
      "really bad.\n",
      "\n",
      "The reason is that a lot of the contexts (word sequences) that the\n",
      "bigram and trigram taggers are based on don't exist in the training set."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Backoff Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To deal with lack of training data for longer contexts, we can use *backoff models*.\n",
      "\n",
      "That is, we first try to assign tags using a complicated model, but if\n",
      "the context is missing, we revert to a smaller, simpler model.\n",
      "\n",
      "This improves the error rate from 10.3% to 7.8%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t0 = nltk.DefaultTagger('NN')\n",
      "t1 = nltk.UnigramTagger(training_set, backoff=t0)\n",
      "t2 = nltk.BigramTagger(training_set, backoff=t1)\n",
      "t2.evaluate(test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "0.9229828275699835"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More context does not actually help."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t3 = nltk.TrigramTagger(training_set,backoff=t2)\n",
      "t3.evaluate(test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "0.9222300635144672"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Intrinsic Error Rate"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We already talked about intrinsic error rates in the context of Bayesian classification.\n",
      "\n",
      "For n-gram taggers, we can compute the error rates more directly.\n",
      "\n",
      "Consider $P(T|W_n,W_{n-1},W_{n-2})$.\n",
      "\n",
      "The rate of correct tagging for a given context is given by $\\max_T P(T|W_n,W_{n-1},W_{n-2})$.\n",
      "\n",
      "The intrinsic error rate for a context is therefore $1-\\max_T P(T|W_n,W_{n-1},W_{n-2})$.\n",
      "\n",
      "If we sum this over all contexts and their probabilities, we get the overall\n",
      "intrinsic error rate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd = nltk.ConditionalFreqDist(\n",
      "           ((x[1], y[1], z[0]), z[1])\n",
      "           for sent in training_set\n",
      "           for x, y, z in nltk.trigrams(sent))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd.conditions()[30000:30003]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "[('ABN', 'CC', 'bear'), ('ABN', 'CC', 'dispensing'), ('ABN', 'CC', 'had')]"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[cfd[cfd.conditions()[i]].items() for i in range(30000,30005)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "[[('VB', 1)], [('VBG', 1)], [('HVD', 1)], [('IN', 1)], [('RB', 1)]]"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ambiguous_contexts[5000]\n",
      "print cfd[ambiguous_contexts[5000]].items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('NNS', ',', 'leaves')\n",
        "[('NNS', 1), ('VBZ', 1)]\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(cfd[c].N() for c in ambiguous_contexts) *1.0 / cfd.N()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "0.11594425738142042"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def words(s): return [w for w,t in s]\n",
      "bigram_tagger.tag(words(test_set[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "[('``', '``'),\n",
        " ('So', 'RB'),\n",
        " (\"you're\", 'PPSS+BER'),\n",
        " ('looking', 'VBG'),\n",
        " ('for', 'IN'),\n",
        " ('a', 'AT'),\n",
        " ('woman', 'NN'),\n",
        " ('who', 'WPS'),\n",
        " ('married', None),\n",
        " ('a', None),\n",
        " ('man', None),\n",
        " ('who', None),\n",
        " ('might', None),\n",
        " ('have', None),\n",
        " ('lived', None),\n",
        " ('here', None),\n",
        " ('a', None),\n",
        " ('year', None),\n",
        " ('ago', None),\n",
        " ('and', None),\n",
        " ('might', None),\n",
        " ('have', None),\n",
        " ('been', None),\n",
        " ('poisoned', None),\n",
        " ('.', None)]"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted_tags = [tag for sent in test_set for (word, tag) in t2.tag(words(sent))]\n",
      "true_tags = [tag for sent in test_set for (word, tag) in sent]\n",
      "confusion = nltk.ConfusionMatrix(true_tags, predicted_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"confusion.txt\",\"w\") as stream: stream.write(str(confusion))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!gvim -c ':set nowrap' confusion.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "conf = Counter(zip(predicted_tags,true_tags))\n",
      "conf.most_common(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "[(('NN', 'NN'), 2707),\n",
        " (('IN', 'IN'), 2076),\n",
        " (('AT', 'AT'), 1847),\n",
        " ((',', ','), 1126),\n",
        " (('.', '.'), 1052),\n",
        " (('JJ', 'JJ'), 1021),\n",
        " (('NNS', 'NNS'), 913),\n",
        " (('CC', 'CC'), 678),\n",
        " (('RB', 'RB'), 574),\n",
        " (('NP', 'NP'), 572)]"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[((u,v),n) for (u,v),n in conf.most_common() if u!=v][:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "[(('TO', 'IN'), 124),\n",
        " (('NN', 'JJ'), 99),\n",
        " (('NN', 'NP'), 79),\n",
        " (('VBN', 'VBD'), 70),\n",
        " (('IN', 'TO'), 69),\n",
        " (('NN', 'VB'), 67),\n",
        " (('NN', 'NNS'), 67),\n",
        " (('VBD', 'VBN'), 66),\n",
        " (('NP', 'NP-TL'), 33),\n",
        " (('JJ', 'NN'), 31)]"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    }
   ],
   "metadata": {}
  }
 ]
}